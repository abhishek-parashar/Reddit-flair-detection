{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "web scrapping",
      "provenance": [],
      "authorship_tag": "ABX9TyNtG5tHlabnXOqUhYxId5g1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhishek-parashar/Reddit-flair-detection/blob/master/web_scrapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnCdGY4gMKoW",
        "colab_type": "text"
      },
      "source": [
        "## scrapping the reddit data from reddit.\n",
        "### references- \n",
        "#### 1. https://www.youtube.com/watch?v=NRgfgtzIhBQ&t=103s\n",
        "#### 2. https://www.storybench.org/how-to-scrape-reddit-with-python/\n",
        "#### 3. https://towardsdatascience.com/scraping-reddit-data-1c0af3040768\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9u9lXAftL2he",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# getting the data from reddit using praw "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMeOmo8XOkFB",
        "colab_type": "code",
        "outputId": "9fbe5501-ba80-40a6-c043-3c3ecf72d0c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "source": [
        "!pip install praw"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: praw in /usr/local/lib/python3.6/dist-packages (6.5.1)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.6/dist-packages (from praw) (0.57.0)\n",
            "Requirement already satisfied: update-checker>=0.16 in /usr/local/lib/python3.6/dist-packages (from praw) (0.16)\n",
            "Requirement already satisfied: prawcore<2.0,>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from praw) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from websocket-client>=0.54.0->praw) (1.12.0)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from update-checker>=0.16->praw) (2.21.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.16->praw) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.16->praw) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.16->praw) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.16->praw) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwGmQ82Z2_-x",
        "colab_type": "text"
      },
      "source": [
        "importing the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCZPzVtoO3Qm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import praw\n",
        "import numpy as np \n",
        "import pandas as pd\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIUsmojcNf88",
        "colab_type": "text"
      },
      "source": [
        "for getting the reddit data from praw we need to enter the credentials"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVmswRpeNe-i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reddit = praw.Reddit(client_id='QPdCUgBcp4WinA', client_secret='HF-sKHVC5Os3gufVxWvzIKijNb4', user_agent='reddit-flair', username='reddit-flair', password='flair123')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uiskvjvZsqF",
        "colab_type": "text"
      },
      "source": [
        "### getting the subreddits and the topics and flairs\n",
        "### labels_dict stores all the parts of reddit post that is flair, id, url, body, comments etc.\n",
        "### flair contails all the list of flairs of the subreddit India."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9heWAt4Zrgl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "subreddit = reddit.subreddit('india')\n",
        "labels_dict = {\"flair\":[], \"title\":[], \"score\":[], \"id\":[], \"url\":[], \"comms_num\": [], \"created\": [], \"body\":[], \"author\":[], \"comments\":[]}\n",
        "flairs = [\"AskIndia\", \"Non-Political\", \"[R]eddiquette\", \"Scheduled\", \"Photography\", \"Science/Technology\", \"Politics\", \"Business/Finance\", \"Policy/Economy\", \"Sports\", \"Food\", \"AMA\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xUWd0nZask6",
        "colab_type": "text"
      },
      "source": [
        "### appending the scrapped data into the dictionary\n",
        "### we iterate through all the flairs and append the url, body, score, flairs etc in the dictionary.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwcfG87yaqzE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for flair in flairs:\n",
        "  \n",
        "  get_subreddits = subreddit.search(flair, limit=100)\n",
        "  \n",
        "  for submission in get_subreddits:\n",
        "    \n",
        "    labels_dict[\"flair\"].append(flair)\n",
        "    labels_dict[\"title\"].append(submission.title)\n",
        "    labels_dict[\"score\"].append(submission.score)\n",
        "    labels_dict[\"id\"].append(submission.id)\n",
        "    labels_dict[\"url\"].append(submission.url)\n",
        "    labels_dict[\"comms_num\"].append(submission.num_comments)\n",
        "    labels_dict[\"created\"].append(submission.created)\n",
        "    labels_dict[\"body\"].append(submission.selftext)\n",
        "    labels_dict[\"author\"].append(submission.author)\n",
        "    \n",
        "    submission.comments.replace_more(limit=None)\n",
        "    comment = ''\n",
        "    for top_level_comment in submission.comments:\n",
        "      comment = comment + ' ' + top_level_comment.body\n",
        "    labels_dict[\"comments\"].append(comment)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awRbYyB21XEN",
        "colab_type": "text"
      },
      "source": [
        "### A Datetime library is used to append the time stamp of the reddit posts. \n",
        "### Created a function to use it further in the data creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOUsx9ABe8Fb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime as dt\n",
        "def get_date(created):\n",
        "    return dt.datetime.fromtimestamp(created)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gxsm3X3gbTNC",
        "colab_type": "text"
      },
      "source": [
        "### converting into dataframe and saving the csv file \n",
        "### pandas is used to create the data frame and then the time stamp is appended by using th function created above. \n",
        "### at last the data is saved in a csv file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg-xy2rta8Dj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.DataFrame(labels_dict)\n",
        "time =data[\"created\"].apply(get_date)\n",
        "data =data.assign(timestamp = time)\n",
        "del data['created']\n",
        "data.to_csv('reddit-india-data.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhLXrCpEfpke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}